{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B2Rqvlj6eTg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from torch.optim import lr_scheduler\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffPDRtcT7ZXF",
        "outputId": "96b38935-5c44-4df2-84c3-3d36979d0d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjAJKhtU7f-D"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = '/content/drive/MyDrive/gpt_checkpoint.pth'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_yl9SDdAWRc",
        "outputId": "e7387130-3bb4-499f-dc51-af9cc9ddf740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdbPBT5q7OAu",
        "outputId": "07c894ef-1d5a-4664-e464-35c5667df442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-11 17:29:54--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-12-11 17:29:54 (18.1 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n",
            "Dataset length: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# Read the dataset\n",
        "with open('input.txt', 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "print(f\"Dataset length: {len(data)} characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpM6p4zK7STK"
      },
      "outputs": [],
      "source": [
        "class CharDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Character-level Dataset for Shakespeare text.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        # Get all unique characters in the data\n",
        "        chars = sorted(list(set(data)))\n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }  # Mapping from character to index\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }  # Mapping from index to character\n",
        "        self.vocab_size = len(chars)\n",
        "        self.block_size = block_size\n",
        "        self.data = data\n",
        "        self.tokenized_data = [self.stoi[c] for c in data]  # Convert all data to indices\n",
        "\n",
        "    def __len__(self):\n",
        "        # Total number of samples\n",
        "        return len(self.tokenized_data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a chunk of (block_size + 1) characters\n",
        "        chunk = self.tokenized_data[idx:idx + self.block_size + 1]\n",
        "        # Input is first n characters, target is next n characters\n",
        "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def decode(self, idx_list):\n",
        "        # Convert a list of indices back to a string\n",
        "        return ''.join([self.itos[i] for i in idx_list])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-scKfBOWOuw",
        "outputId": "8c8ca9ce-b960-464c-cd4c-bd2bf9f4a97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 65\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Hyperparameters\n",
        "block_size = 256  # Length of each input sequence\n",
        "batch_size = 128  # Number of sequences per batch\n",
        "\n",
        "# Create dataset instance\n",
        "dataset = CharDataset(data, block_size)\n",
        "vocab_size = dataset.get_vocab_size()\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "\n",
        "chunk_size = 1000\n",
        "buffer_size = 50\n",
        "\n",
        "chunks = []\n",
        "for i in range(0, len(data), chunk_size + buffer_size):\n",
        "    chunk = data[i:i+chunk_size]\n",
        "    if len(chunk) == chunk_size:\n",
        "        chunks.append(chunk)\n",
        "\n",
        "\n",
        "split_idx = int(0.9 * len(chunks))\n",
        "train_chunks = chunks[:split_idx]\n",
        "val_chunks = chunks[split_idx:]\n",
        "\n",
        "\n",
        "train_data = ''.join(train_chunks)\n",
        "val_data = ''.join(val_chunks)\n",
        "\n",
        "def augment_data(data, prob=0.04):\n",
        "    augmented = []\n",
        "    for char in data:\n",
        "        if random.random() < prob:\n",
        "\n",
        "            augmented.append(random.choice(list(set(data))))\n",
        "        else:\n",
        "            augmented.append(char)\n",
        "    return ''.join(augmented)\n",
        "\n",
        "\n",
        "train_data = augment_data(train_data)\n",
        "\n",
        "\n",
        "train_dataset = CharDataset(train_data, block_size)\n",
        "val_dataset = CharDataset(val_data, block_size)\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2vPvJW77hoc"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    vocab_size = vocab_size\n",
        "    n_embed = 768       # Embedding dimension\n",
        "    n_head = 8          # Number of attention heads\n",
        "    n_layer = 12        # Number of transformer blocks\n",
        "    block_size = block_size\n",
        "    dropout = 0.3       # Dropout rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3-SquN67kUQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embed % config.n_head == 0\n",
        "\n",
        "        self.n_head = config.n_head\n",
        "        self.head_dim = config.n_embed // config.n_head\n",
        "\n",
        "        self.key = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.query = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.value = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.attn_drop = nn.Dropout(config.dropout)\n",
        "        self.proj = nn.Linear(config.n_embed, config.n_embed)\n",
        "        self.proj_drop = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Causal mask to ensure attention only to previous positions\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # Linear projections\n",
        "        k = self.key(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        q = self.query(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "        v = self.value(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        attn_weights = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Apply causal mask\n",
        "        attn_weights = attn_weights.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "\n",
        "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
        "        attn_probs = self.attn_drop(attn_probs)\n",
        "\n",
        "        y = attn_probs @ v  # Combine attention and values\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.proj(y)\n",
        "        y = self.proj_drop(y)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFYj4BnM7mpj"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embed)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embed)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embed, 4 * config.n_embed),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.n_embed, config.n_embed),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))  # Residual connection around attention\n",
        "        x = x + self.mlp(self.ln2(x))   # Residual connection around MLP\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmAmgs-h7ok_"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(config.vocab_size, config.n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(config.block_size, config.n_embed)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embed)\n",
        "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        print(f\"Number of parameters: {sum(p.numel() for p in self.parameters())}\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "\n",
        "        # Token and position embeddings\n",
        "        token_embeddings = self.token_embedding_table(idx)\n",
        "        position_ids = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        position_embeddings = self.position_embedding_table(position_ids)\n",
        "\n",
        "        x = self.dropout(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Compute cross-entropy loss\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "\n",
        "            idx_cond = idx[:, -self.config.block_size:]  # Ensure input is within block size\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]  # Focus on the last time step\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(GPT.generate.__doc__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z94tFQ4oqJ5I",
        "outputId": "7babd2de-ee9d-4083-8149-4ac38e397a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9T0wBi00DaZ",
        "outputId": "979e3283-bb18-4c09-f363-b5d093d91372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 85352513\n"
          ]
        }
      ],
      "source": [
        "config = Config()\n",
        "config.vocab_size = len(train_dataset.stoi)\n",
        "model = GPT(config).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x11lYqv970yL"
      },
      "outputs": [],
      "source": [
        "#def load_model(path, model):\n",
        "    #if os.path.exists(path):\n",
        "     #   model.load_state_dict(torch.load(path))\n",
        "      #  print(f'Model loaded from {path}')\n",
        "   # else:\n",
        "       # print(f'No checkpoint found at {path}')\n",
        "    #return model\n",
        "\n",
        "#model = load_model(checkpoint_path, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mecpWDk7szL"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5,weight_decay=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n"
      ],
      "metadata": {
        "id": "CJjDyffxvOsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUP_AJRE7u90",
        "outputId": "b1e3ea8c-c0c0-4bdc-d109-6970dcc7d123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7458/7458 [29:59<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已保存到 /content/drive/MyDrive/gpt_checkpoint.pth\n",
            "Epoch 1/10, Training Loss: 2.9658, Validation Loss: 4.3851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7458/7458 [30:00<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已保存到 /content/drive/MyDrive/gpt_checkpoint.pth\n",
            "Epoch 2/10, Training Loss: 3.1321, Validation Loss: 4.2487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7458/7458 [30:01<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已保存到 /content/drive/MyDrive/gpt_checkpoint.pth\n",
            "Epoch 3/10, Training Loss: 2.9470, Validation Loss: 4.5903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7458/7458 [29:59<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已保存到 /content/drive/MyDrive/gpt_checkpoint.pth\n",
            "Epoch 4/10, Training Loss: 3.2772, Validation Loss: 4.1892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7458/7458 [30:01<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已保存到 /content/drive/MyDrive/gpt_checkpoint.pth\n",
            "Epoch 5/10, Training Loss: 2.9157, Validation Loss: 4.6900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7458/7458 [30:02<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "模型已保存到 /content/drive/MyDrive/gpt_checkpoint.pth\n",
            "Epoch 6/10, Training Loss: 2.4636, Validation Loss: 4.9849\n",
            "Early stopping!\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from torch.amp import autocast, GradScaler\n",
        "scaler = GradScaler(init_scale=2.0)\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "patience = 2\n",
        "trigger_times = 0\n",
        "loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x, y in tqdm(train_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(device_type='cuda'):\n",
        "            logits, loss = model(x, y)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f'The model has been saved to： {checkpoint_path}')\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            logits, loss = model(x, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        trigger_times = 0\n",
        "\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print('Early stopping!')\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz8yG63X7xEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31756a9e-1c02-46c1-af11-e9954f27b6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O God, O God!\n",
            "LORK:\n",
            "Prat that come thou upee? O, who see yin him; all there.\n",
            "\n",
            "FRIV:\n",
            "'Thas speast we boer's ancuse Yizes.\n",
            "\n",
            "FLORD:You, bearqut wath delus .lke feefe! tor beatt Bloord;\n",
            "Wntagh I on spilege? ther side grom;\n",
            "This for your fothe both Frim.\n",
            "\n",
            "HENRY MIO:\n",
            "Shan well if thow detustutor?\n",
            "\n",
            "CAMILLLLLO:\n",
            "Gos to seeees broveds? earje the stes hose:\n",
            "What your the nim Cuppt and thou ford tate;\n",
            "Cuchan sir, al let befor is sSrliestifed prowng,\n",
            "That mist soomet dith.'\n",
            "\n",
            "CORIOLANUS:\n",
            "Nless? lik, shisch loom, andgean\n",
            "O\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    context = \"O God, O God!\"\n",
        "    context_idx = torch.tensor([dataset.stoi[c] for c in context], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    generated_idx = model.generate(context_idx, max_new_tokens=500)[0].tolist()\n",
        "    completion = dataset.decode(generated_idx)\n",
        "    print(completion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHw1Dwwl70Ou"
      },
      "outputs": [],
      "source": [
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs * len(train_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGzWIBJM73hz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e485770-da54-4018-df61-57cd17cd75d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.3698, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eIMfXFu75wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3dc4ca-1484-4f6b-cd02-7e0ba8a3e3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Perplexity: 146.19\n"
          ]
        }
      ],
      "source": [
        "def compute_perplexity(model, data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in data_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, loss = model(x, y)\n",
        "            total_loss += loss.item() * x.size(0) * x.size(1)\n",
        "            total_tokens += x.size(0) * x.size(1)\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return perplexity\n",
        "\n",
        "val_perplexity = compute_perplexity(model, val_loader)\n",
        "print(f\"Validation Perplexity: {val_perplexity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhStpANh774b"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'gpt_shakespeare.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwrz-03j79aw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df43086-9ab9-4c0b-8d32-b79b6acedec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-b97e1665f218>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('gpt_shakespeare.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Load the model\n",
        "model.load_state_dict(torch.load('gpt_shakespeare.pth'))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
